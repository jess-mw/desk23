# Data Model

This document shows the design and evolution of our data model.

*	Suggesting MongoDB for storage
*	Front end consumes data from own API and database, but can populate your own database with other databases using functions within node
*	Think about way that data is transferred between front and back

What are the entities in the system?
*	Whole globe
*	Each country
*	Welcome page


Model
*	Look up information stored on welcome page
*	Take in information about where the user’s mouse is
*	Look up overview statistics for a county
*	Take in information about what the user does next – whether they click on a country or move mouse to hover over another country
*	Look up detailed statistics for a country
*	Take in information about whether the user exists these details
*	Reading data of statistics for each country, and then storing that somewhere it can be accessed to adjust the display of countries (i.e. how dark or bright they are)

## Building the database 

* Our initial data was stored in a csv file pulled from an online source 
* This data was adjusted to have simpler headings allowing for easier manipulation 
* Data model was created to describe schema for this database 
* At this point, only one table so not too difficult to implement 
* Database was created and linked to the docker-compose 
* Created a JavaScript file called db.js that is run in the docker-compose 
* This forms a connection to the database, and creates a model of our table 
* Our CSV file was transformed into a js object 
* This object is exported to db.js and inserted into our data model 
* The connection is then disconnected from the database 
* “basically i converted the csv file into a json using csvtojson on the command line, then went into the file and turned it into a js object. in db.js I then include it as a variable and use Model.insertMany(variablewithdata)” 
* Now the method is understood, it should be easy to re-implement with other csv files 
* Have a series of query helper functions that get relevant/required data 
* Realised that for front end it will be much easier to have one collection containing both literacy rates and coordinates 
* Looked into doing embedded documents, however as we were inserting the data from json files, this wasn’t simple 
* Also as they were files we had lifted off of the internet, there were issues with foreign keys linking the collections being spelled differently, or present in one file but not the other 
* Decided to create a new collection that had all the information we needed; this also provided a good opportunity to clean the data and remove any unnecessary noise 

## Issues: 

* Getting the data from the CSV file into the database proved to be challenging and took much longer than expected, due to limitations in conceptual understanding 
* Unsure whether need to run create model every time we run the code or only once - found out it was uploading every time so deleted database collections and restarted to get rid of duplicates 
* However, sometimes we found that when loaded the database was empty and we weren't sure why - want to avoid populating the database every time and having an empty database
* Finding a good way to link tables, especially considering they were from datasets that we did not create 
* Removing noise from the datasets, as we did not create them ourselves
* Synchronising the different datasets that were pulled from the internet

## Data Model - First Iteration
![image](https://user-images.githubusercontent.com/45073537/116996776-d82f7d80-acd3-11eb-9629-774d2931b08d.png)

## Data Model - Second Iteration
![image](https://user-images.githubusercontent.com/45073537/117008188-5c88fd00-ace2-11eb-861b-06f52baed76d.png)

## Data Model - Third Iteration
![image](https://user-images.githubusercontent.com/45073537/117008237-6b6faf80-ace2-11eb-8271-73d8342239c8.png)
